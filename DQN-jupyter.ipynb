{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN Submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# Install dependancies\n",
        "\n",
        "Rendering Dependancies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-1LTSH88EE",
        "colab_type": "text"
      },
      "source": [
        "Gym Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import time\n",
        "from skimage.transform import downscale_local_mean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "      mp4 = mp4list[0]\n",
        "      video = io.open(mp4, 'r+b').read()\n",
        "      encoded = base64.b64encode(video)\n",
        "      ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                  loop controls style=\"height: 400px;\">\n",
        "                  <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "              </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "      print(\"Could not find video\")\n",
        "      \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUB4koUFuyJH",
        "colab_type": "text"
      },
      "source": [
        "# Program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zusUh1Du6FD",
        "colab_type": "text"
      },
      "source": [
        "Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxUqr7I1uw0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Q_Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Q_Network, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc1 = nn.Linear(4 * 64 * 7 * 7, 512)   # [(W-K+2P)/S]+1\n",
        "        self.fc2 = nn.Linear(512, 6)\n",
        "\n",
        "        self.device = T.device('cuda')\n",
        "        self.to(self.device)\n",
        "        \n",
        "    def forward(self, batch):\n",
        "        \n",
        "        flatten = torch.Tensor()\n",
        "        \n",
        "        f0 = batch[:, 0, :, :].view(-1, 1, 84,84)\n",
        "        f1 = batch[:, 1, :, :].view(-1, 1, 84,84)\n",
        "        f2 = batch[:, 2, :, :].view(-1, 1, 84,84)\n",
        "        f3 = batch[:, 3, :, :].view(-1, 1, 84,84)\n",
        "        \n",
        "        f0 = F.relu(self.conv1(f0))\n",
        "        f1 = F.relu(self.conv1(f1))\n",
        "        f2 = F.relu(self.conv1(f2))\n",
        "        f3 = F.relu(self.conv1(f3))\n",
        "        \n",
        "        f0 = F.relu(self.conv2(f0))\n",
        "        f1 = F.relu(self.conv2(f1))\n",
        "        f2 = F.relu(self.conv2(f2))\n",
        "        f3 = F.relu(self.conv2(f3))\n",
        "        \n",
        "        f0 = F.relu(self.conv3(f0))\n",
        "        f1 = F.relu(self.conv3(f1))\n",
        "        f2 = F.relu(self.conv3(f2))\n",
        "        f3 = F.relu(self.conv3(f3))\n",
        "        \n",
        "        f0 = f0.view(-1, 64 * 7 * 7)\n",
        "        f1 = f1.view(-1, 64 * 7 * 7)\n",
        "        f2 = f2.view(-1, 64 * 7 * 7)\n",
        "        f3 = f3.view(-1, 64 * 7 * 7)\n",
        "        flatten = torch.cat( (f0, f1, f2, f3), dim=1 )\n",
        "        \n",
        "        fc = F.relu(self.fc1(flatten))\n",
        "        action_values = self.fc2(fc)\n",
        "        \n",
        "        return action_values    # [Q action_values] * frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvzAp9H_u8LT",
        "colab_type": "text"
      },
      "source": [
        "Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQO3H7x4u8Y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self):\n",
        "        self.buffer = []\n",
        "        self.max_length = 50000\n",
        "\n",
        "    def push(self, data):\n",
        "        self.buffer.append( data )\n",
        "        if len(self.buffer) > self.max_length:\n",
        "          del self.buffer[0]\n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        minibatch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        states = np.array([i[0] for i in minibatch]).astype('float32')\n",
        "        next_states = np.array([i[1] for i in minibatch]).astype('float32')\n",
        "        actions = np.array([i[2] for i in minibatch]).astype('float32')\n",
        "        rewards = np.array([i[3] for i in minibatch]).astype('float32')\n",
        "        terminals = np.array([i[4] for i in minibatch]).astype('float32')\n",
        "        \n",
        "        return  T.from_numpy(states).cuda(), \\\n",
        "                T.from_numpy(next_states).cuda(), \\\n",
        "                T.from_numpy(actions).cuda(), \\\n",
        "                T.from_numpy(rewards).cuda(), \\\n",
        "                T.from_numpy(terminals).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXLauMPWvCUb",
        "colab_type": "text"
      },
      "source": [
        "Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFuab8JbvDd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent(object):\n",
        "    def __init__(self, model, optim):\n",
        "        self.model = model\n",
        "        self.target = copy.deepcopy(self.model)\n",
        "        self.optim = optim\n",
        "        self.gamma = 0.99\n",
        "        self.huber_loss = nn.SmoothL1Loss()\n",
        "        self.replay_buffer = ReplayBuffer()\n",
        "        self.tau = 0.05\n",
        "        pass\n",
        "    \n",
        "    def update(self):\n",
        "        for params, target_params in zip(self.model.parameters(), self.target.parameters()):\n",
        "            target_params.data.copy_(params.data * self.tau + target_params.data * (1 - self.tau))\n",
        "    \n",
        "    def select_action(self, state, epsilon):\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            return env.action_space.sample()  # choose random action\n",
        "        else:\n",
        "            t_state = T.from_numpy(state[np.newaxis,:,:,:]).cuda()\n",
        "            qvalues = self.model(t_state.float()).cuda()\n",
        "            qvalues = qvalues.cpu().data.numpy()\n",
        "            return np.argmax(qvalues)  # choose greedy action\n",
        "\n",
        "    def learn(self, batch_size):\n",
        "        \n",
        "        if(len(self.replay_buffer.buffer) < batch_size):\n",
        "            return\n",
        "        \n",
        "        states, next_states, actions, rewards, dones = self.replay_buffer.sample(batch_size)\n",
        "        \n",
        "        state_qvalues = T.gather(self.model(states).cuda(), dim=1, index=actions.long()).cuda()\n",
        "        next_state_qvalues = self.target(next_states).cuda()\n",
        "        next_actions, _ = T.max(next_state_qvalues, dim=1, keepdim=True)\n",
        "        not_dones = 1 - dones\n",
        "        target_values = rewards + not_dones * self.gamma * next_actions\n",
        "        loss = self.huber_loss(state_qvalues, target_values.detach())\n",
        "        self.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optim.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_dVb3_MvF00",
        "colab_type": "text"
      },
      "source": [
        "Helper Funtions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBZOMZBRvIpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(frame):\n",
        "    r = frame[34:194,:,0]\n",
        "    g = frame[34:194,:,1]\n",
        "    b = frame[34:194,:,2]\n",
        "    frame = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "    frame = np.pad(frame, (4,4), mode='edge')\n",
        "    frame = downscale_local_mean(frame, (2, 2))\n",
        "    \n",
        "    frame = frame[np.newaxis,:,:]\n",
        "    \n",
        "    return frame\n",
        " \n",
        "def generate_test_set(env):\n",
        "        \n",
        "    done = False\n",
        "    frame = preprocess(env.reset())\n",
        "    state = np.repeat(frame, 4, axis=0)\n",
        "    states = []\n",
        "    \n",
        "    while not done:\n",
        "        frame, reward, done, _ = env.step(env.action_space.sample())\n",
        "        \n",
        "        frame = preprocess(frame)\n",
        "        next_state = np.delete(state, 0, 0)\n",
        "        next_state = np.concatenate((next_state, frame), axis=0)\n",
        "        \n",
        "        states.append(next_state)\n",
        "        state = next_state\n",
        "        \n",
        "    return torch.tensor(states, device=cuda).float()\n",
        "\n",
        "def eval_model(data, model):\n",
        "    predictions = model(data)\n",
        "    return predictions.cpu().data.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9F9BrwqvKna",
        "colab_type": "text"
      },
      "source": [
        "GPU Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiW60fOqxdUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device_number = T.cuda.current_device()\n",
        "print('Cuda avaibale:', T.cuda.is_available())\n",
        "print('Cuda devices:', T.cuda.device_count())\n",
        "print('Name:', T.cuda.get_device_name(device_number))\n",
        "\n",
        "cuda = T.device('cuda')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF5cC9ezEoS3",
        "colab_type": "text"
      },
      "source": [
        "Link Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ye8v-_1Equo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNZFAJE5FCr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls '/content/gdrive/My Drive/Models'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl2NeIFTtuXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKbVZEVjtult",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlwmngDTtuuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94NGovfJxfXY",
        "colab_type": "text"
      },
      "source": [
        "Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL604vJ4reZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"PongDeterministic-v4\")\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "model = Q_Network()\n",
        "optimizer = T.optim.Adam( model.parameters(), lr=0.0001)\n",
        "    \n",
        "agent = DQNAgent(model, optimizer)\n",
        "\n",
        "episode = 0\n",
        "step = 0\n",
        "plotting = False \n",
        "\n",
        "max_time_steps = 5000000\n",
        "batch_size = 32\n",
        "epsilon_decay = 0.9 / 500000\n",
        "epsilon = 1 - (step * epsilon_decay)\n",
        "epsilon_min = 0.1\n",
        "\n",
        "rewards = []\n",
        "averages = []\n",
        "epsilon_history = []\n",
        "step_history = []\n",
        "\n",
        "while step < max_time_steps:\n",
        "    print(\"\\nEpisode: {ep:3d} - Epsilon: {eps:.7f}\".format(ep=episode, eps=epsilon))\n",
        "    \n",
        "    reward_sum = 0\n",
        "    frame = preprocess(env.reset())\n",
        "    state = np.repeat(frame, 4, axis=0)\n",
        "    terminal = False\n",
        "    step_history.append(step)\n",
        "    \n",
        "    while not terminal:\n",
        "        \n",
        "        action = agent.select_action(state , epsilon)\n",
        "        frame, reward, terminal, _ = env.step(action)\n",
        "        \n",
        "        frame = preprocess(frame)\n",
        "        next_state = np.delete(state, 0, 0)\n",
        "        next_state = np.concatenate((next_state, frame), axis=0)\n",
        "        \n",
        "        reward_sum += reward\n",
        "        \n",
        "        agent.replay_buffer.push( (state, next_state, [action], [reward], [terminal]) )\n",
        "        state = next_state\n",
        "            \n",
        "        agent.learn(batch_size)\n",
        "        if step % 32 == 0:\n",
        "            agent.update()\n",
        "        \n",
        "        step += 1\n",
        "\n",
        "        if step % 25000 == 0:\n",
        "          plotting = True\n",
        "\n",
        "        if epsilon > epsilon_min:\n",
        "          epsilon -= epsilon_decay \n",
        "        \n",
        "    rewards.append(reward_sum)\n",
        "    average = np.mean(rewards[-100:])\n",
        "    averages.append(average)\n",
        "    epsilon_history.append(epsilon)\n",
        "\n",
        "    print(\"Average over last 100 episode: {0:.2f}\".format(average))\n",
        "    print(\"Reward =\", reward_sum)\n",
        "    print(\"           ---- Steps =\", step, \"----\\n\")\n",
        "\n",
        "    if plotting:\n",
        "        plotting = False\n",
        "        fig, ax1 = plt.subplots()\n",
        "        ax1.plot(step_history, rewards, label = \"Rewards\")\n",
        "        ax1.plot(step_history, averages, label = \"Averages\")\n",
        "        ax2 = ax1.twinx()\n",
        "        ax2.plot(step_history, epsilon_history, label = \"Epsilon\", color = 'red')\n",
        "        ax2.set_ylabel('Epsilon')\n",
        "        ax1.set_xlabel('Network Updates')\n",
        "        plt.show()\n",
        "        print(\"\\n         -------------------------------     \\n\")\n",
        "\n",
        "    episode += 1\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tItVM7uhthaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}